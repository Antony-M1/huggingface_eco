{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**README**\n\nThis document is prepared by the **Kaggle**.\n","metadata":{}},{"cell_type":"markdown","source":"# Install Packages","metadata":{}},{"cell_type":"code","source":"!pip install -q trl","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:54:00.905086Z","iopub.execute_input":"2024-06-26T10:54:00.905438Z","iopub.status.idle":"2024-06-26T10:54:15.875775Z","shell.execute_reply.started":"2024-06-26T10:54:00.905408Z","shell.execute_reply":"2024-06-26T10:54:15.874810Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:55:59.675430Z","iopub.execute_input":"2024-06-26T10:55:59.675756Z","iopub.status.idle":"2024-06-26T10:56:16.168787Z","shell.execute_reply.started":"2024-06-26T10:55:59.675732Z","shell.execute_reply":"2024-06-26T10:56:16.168017Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-06-26 10:56:07.028762: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-26 10:56:07.028871: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-26 10:56:07.157658: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:55:05.282897Z","iopub.execute_input":"2024-06-26T10:55:05.283624Z","iopub.status.idle":"2024-06-26T10:55:05.288232Z","shell.execute_reply.started":"2024-06-26T10:55:05.283592Z","shell.execute_reply":"2024-06-26T10:55:05.287190Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"login(user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\"))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:55:07.166853Z","iopub.execute_input":"2024-06-26T10:55:07.167695Z","iopub.status.idle":"2024-06-26T10:55:07.410790Z","shell.execute_reply.started":"2024-06-26T10:55:07.167664Z","shell.execute_reply":"2024-06-26T10:55:07.409842Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# [TRL](https://pypi.org/project/trl/#description)\n\nTRL - Transformer Reinforcement Learning\n\n**Full stack library to fine-tune and align large language models.**\n\n**What is it?**\n\nThe trl library is a full stack tool to fine-tune and align transformer language and diffusion models using methods such as `Supervised Fine-tuning` step (SFT), `Reward Modeling` (RM) and the `Proximal Policy Optimization` (PPO) as well as `Direct Preference Optimization` (DPO).\n\nThe library is built on top of the `transformers` library and thus allows to use any model architecture available there.\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:25:06.186183Z","iopub.execute_input":"2024-06-26T09:25:06.186931Z","iopub.status.idle":"2024-06-26T09:25:06.194993Z","shell.execute_reply.started":"2024-06-26T09:25:06.186892Z","shell.execute_reply":"2024-06-26T09:25:06.193787Z"}}},{"cell_type":"markdown","source":"## Highlights\n\n* **Efficient and scalable**\n\n  * `accelerate` is the backbone of trl which allows to scale model training from a single GPU to a large scale multi-node cluster with methods such as `DDP` and `DeepSpeed`.\n  * `PEFT` is fully integrated and allows to train even the largest models on modest hardware with quantisation and methods such as **LoRA** or **QLoRA**.\n  * `unsloth` is also integrated and allows to significantly speed up training with dedicated kernels.\n\n* `CLI`: With the CLI you can fine-tune and chat with LLMs without writing any code using a single command and a flexible config system.\n\n* `Trainers`: The Trainer classes are an abstraction to apply many fine-tuning methods with ease such as the `SFTTrainer`, `DPOTrainer`, `RewardTrainer`, `PPOTrainer`, `CPOTrainer`, and `ORPOTrainer`.\n\n* `AutoModels`: The `AutoModelForCausalLMWithValueHead` & `AutoModelForSeq2SeqLMWithValueHead` classes add an additional value head to the model which allows to train them with **RL** algorithms such as *PPO*.\n\n* `Examples`: Train GPT2 to generate positive movie reviews with a BERT sentiment classifier, full RLHF using adapters only, train GPT-j to be less toxic, StackLlama example, etc. following the examples.\n","metadata":{}},{"cell_type":"markdown","source":"## Command Line Interface (CLI)\n\nYou can use TRL Command Line Interface (CLI) to quickly get started with Supervised Fine-tuning (SFT), Direct Preference Optimization (DPO) and test your aligned model with the chat CLI:","metadata":{}},{"cell_type":"markdown","source":"1. **SFT - Supervised Fine Tuning**\n```\n!trl sft --model_name_or_path facebook/opt-125m --dataset_name imdb --output_dir opt-sft-imdb\n```\n2. **DPO - Direct Preference Optimization**\n```\n!trl dpo --model_name_or_path facebook/opt-125m --dataset_name trl-internal-testing/hh-rlhf-helpful-base-trl-style --output_dir opt-sft-hh-rlhf\n```\n3. **Chat**\n```\n!trl chat --model_name_or_path Qwen/Qwen1.5-0.5B-Chat\n```\n\nThe above three commands we can use to Run & Train a model through `trl` command.\n\nI will run one chat command below for testing.","metadata":{}},{"cell_type":"code","source":"# !trl chat --model_name_or_path Qwen/Qwen1.5-0.5B-Chat # remove the comment and run","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to use\n\nFor more flexibility and control over the training, you can use the dedicated trainer classes to fine-tune the model in Python.","metadata":{}},{"cell_type":"markdown","source":"### SFTTrainer\n\nThis is a basic example of how to use the `SFTTrainer` from the library. The `SFTTrainer` is a light wrapper around the transformers Trainer to easily fine-tune language models or adapters on a `custom dataset`.\n\n**SFTTrainer** stands for `Supervised Fine-Tuning Trainer`. It's a class provided by the TRL package that facilitates the supervised fine-tuning of transformer models. This class helps in training a transformer model on a specific dataset using supervised learning techniques.\n\n**Main Use of SFTTrainer**\n\n1. `Fine-Tuning Pretrained Models`: SFTTrainer is primarily used to fine-tune pre-trained transformer models (e.g., BERT, GPT) on a specific dataset. Fine-tuning is a crucial step in adapting a general-purpose pre-trained model to a specific task, such as text classification, named entity recognition, or machine translation.\n2. `Supervised Learning`: It enables supervised learning by providing the necessary methods to train a model using labeled data. This involves defining a loss function, optimizing the model parameters, and evaluating the model's performance on validation data.\n3. `Customization`: SFTTrainer allows for customization of the training process. Users can specify their own loss functions, optimization algorithms, and other training parameters to suit their specific needs.\n\n**Where to Use SFTTrainer**\n\n1. `Natural Language Processing Tasks`: Any NLP task that can benefit from fine-tuning a transformer model can use SFTTrainer. Examples include sentiment analysis, text summarization, question answering, and more.\n\n2. `Custom NLP Pipelines`: If you're building a custom NLP pipeline and need to adapt a transformer model to your specific dataset or task, SFTTrainer can be an essential tool.\n\n3. `Research and Development`: For researchers experimenting with new models or techniques, SFTTrainer provides a flexible and easy-to-use framework for fine-tuning transformer models.\n\n","metadata":{}},{"cell_type":"code","source":"# get staset\ndataset = load_dataset(\"imdb\", split=\"train\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:55:14.303857Z","iopub.execute_input":"2024-06-26T10:55:14.304232Z","iopub.status.idle":"2024-06-26T10:55:19.704134Z","shell.execute_reply.started":"2024-06-26T10:55:14.304202Z","shell.execute_reply":"2024-06-26T10:55:19.703243Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f426af0dfab4fb49d526a7e5bbc3d81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d4f2111d984503a355cfa94d08c61a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d661e4bdaa384d33abb2e0519dbbb9fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cc39e2d4edd404791ad8019a1d7d128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a21078a1e4f74210ac2690da27e19c25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17fd6383476f47a1951fbc65fe9567ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f49b5fc6e54e769bde1d49cf7f2823"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})"},"metadata":{}}]},{"cell_type":"code","source":"# get trainer\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T10:56:26.236775Z","iopub.execute_input":"2024-06-26T10:56:26.237477Z","iopub.status.idle":"2024-06-26T10:56:45.541541Z","shell.execute_reply.started":"2024-06-26T10:56:26.237443Z","shell.execute_reply":"2024-06-26T10:56:45.540770Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:146: UserWarning: No `SFTConfig` passed, using `output_dir=tmp_trainer`.\n  warnings.warn(f\"No `SFTConfig` passed, using `output_dir={output_dir}`.\")\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:174: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f6d1e276b84c8d8b68e2dcab7eb598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07f3906f0a5d4069be47ab4319e03a04"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5259fc60ef53433f836589f1f95486ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be132270215948a4ab7d1b2def3f4518"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95510b058a94ca9bac9a46adec4415f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54ce411707a423dafdcbe3dd2c790fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0493c330f7d492e8920fb8458bdb440"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12d0a5476e134cd5b663974eadfe04d1"}},"metadata":{}}]},{"cell_type":"code","source":"# train\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:43:57.690328Z","iopub.execute_input":"2024-06-26T11:43:57.690683Z","iopub.status.idle":"2024-06-26T11:43:58.026804Z","shell.execute_reply.started":"2024-06-26T11:43:57.690639Z","shell.execute_reply":"2024-06-26T11:43:58.025442Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]}]}