{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**README**\n\nThis document is prepared by the **Kaggle**.\n","metadata":{}},{"cell_type":"markdown","source":"# Install Packages","metadata":{}},{"cell_type":"code","source":"!pip install -q trl","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:21:16.376020Z","iopub.execute_input":"2024-06-26T09:21:16.376430Z","iopub.status.idle":"2024-06-26T09:21:31.211638Z","shell.execute_reply.started":"2024-06-26T09:21:16.376400Z","shell.execute_reply":"2024-06-26T09:21:31.210459Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom huggingface_hub import login","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:21:11.372557Z","iopub.execute_input":"2024-06-26T09:21:11.373436Z","iopub.status.idle":"2024-06-26T09:21:11.377565Z","shell.execute_reply.started":"2024-06-26T09:21:11.373403Z","shell.execute_reply":"2024-06-26T09:21:11.376478Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# [TRL](https://pypi.org/project/trl/#description)\n\nTRL - Transformer Reinforcement Learning\n\n**Full stack library to fine-tune and align large language models.**\n\n**What is it?**\n\nThe trl library is a full stack tool to fine-tune and align transformer language and diffusion models using methods such as `Supervised Fine-tuning` step (SFT), `Reward Modeling` (RM) and the `Proximal Policy Optimization` (PPO) as well as `Direct Preference Optimization` (DPO).\n\nThe library is built on top of the `transformers` library and thus allows to use any model architecture available there.\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:25:06.186183Z","iopub.execute_input":"2024-06-26T09:25:06.186931Z","iopub.status.idle":"2024-06-26T09:25:06.194993Z","shell.execute_reply.started":"2024-06-26T09:25:06.186892Z","shell.execute_reply":"2024-06-26T09:25:06.193787Z"}}},{"cell_type":"markdown","source":"## Highlights\n\n* **Efficient and scalable**\n\n  * `accelerate` is the backbone of trl which allows to scale model training from a single GPU to a large scale multi-node cluster with methods such as `DDP` and `DeepSpeed`.\n  * `PEFT` is fully integrated and allows to train even the largest models on modest hardware with quantisation and methods such as **LoRA** or **QLoRA**.\n  * `unsloth` is also integrated and allows to significantly speed up training with dedicated kernels.\n\n* `CLI`: With the CLI you can fine-tune and chat with LLMs without writing any code using a single command and a flexible config system.\n\n* `Trainers`: The Trainer classes are an abstraction to apply many fine-tuning methods with ease such as the `SFTTrainer`, `DPOTrainer`, `RewardTrainer`, `PPOTrainer`, `CPOTrainer`, and `ORPOTrainer`.\n\n* `AutoModels`: The `AutoModelForCausalLMWithValueHead` & `AutoModelForSeq2SeqLMWithValueHead` classes add an additional value head to the model which allows to train them with **RL** algorithms such as *PPO*.\n\n* `Examples`: Train GPT2 to generate positive movie reviews with a BERT sentiment classifier, full RLHF using adapters only, train GPT-j to be less toxic, StackLlama example, etc. following the examples.\n","metadata":{}},{"cell_type":"markdown","source":"## Command Line Interface (CLI)\n\nYou can use TRL Command Line Interface (CLI) to quickly get started with Supervised Fine-tuning (SFT), Direct Preference Optimization (DPO) and test your aligned model with the chat CLI:","metadata":{}},{"cell_type":"markdown","source":"1. **SFT - Supervised Fine Tuning**\n```\n!trl sft --model_name_or_path facebook/opt-125m --dataset_name imdb --output_dir opt-sft-imdb\n```\n2. **DPO - Direct Preference Optimization**\n```\n!trl dpo --model_name_or_path facebook/opt-125m --dataset_name trl-internal-testing/hh-rlhf-helpful-base-trl-style --output_dir opt-sft-hh-rlhf\n```\n3. **Chat**\n```\n!trl chat --model_name_or_path Qwen/Qwen1.5-0.5B-Chat\n```\n\nThe above three commands we can use to Run & Train a model through `trl` command.\n\nI will run one chat command below for testing.","metadata":{}},{"cell_type":"code","source":"# !trl chat --model_name_or_path Qwen/Qwen1.5-0.5B-Chat","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:43:54.724451Z","iopub.execute_input":"2024-06-26T09:43:54.724870Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[2K\u001b[32mтаз\u001b[0m \u001b[1;35mWelcome! Initializing the TRL CLI...\u001b[0m0m\n\u001b[1A\u001b[2K2024-06-26 09:44:02.753251: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-26 09:44:02.753308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-26 09:44:02.754763: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\u001b[2J\u001b[H\u001b[1;31m<\u001b[0m\u001b[1;31mroot\u001b[0m\u001b[1;31m>\u001b[0m\u001b[1;31m:\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}