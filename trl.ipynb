{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**README**\n\nThis document is prepared by the **Kaggle**.\n","metadata":{}},{"cell_type":"markdown","source":"# Install Packages","metadata":{}},{"cell_type":"code","source":"!pip install -q trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# instantiate a distribution strategy\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = tf.keras.Sequential( … ) # define your model normally\n    model.compile( … )\n\n# train model normally\nmodel.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nos.environ[\"HT_TOKEN\"] = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"login(user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [TRL](https://pypi.org/project/trl/#description)\n\nTRL - Transformer Reinforcement Learning\n\n**Full stack library to fine-tune and align large language models.**\n\n**What is it?**\n\nThe trl library is a full stack tool to fine-tune and align transformer language and diffusion models using methods such as `Supervised Fine-tuning` step (SFT), `Reward Modeling` (RM) and the `Proximal Policy Optimization` (PPO) as well as `Direct Preference Optimization` (DPO).\n\nThe library is built on top of the `transformers` library and thus allows to use any model architecture available there.\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:25:06.186183Z","iopub.execute_input":"2024-06-26T09:25:06.186931Z","iopub.status.idle":"2024-06-26T09:25:06.194993Z","shell.execute_reply.started":"2024-06-26T09:25:06.186892Z","shell.execute_reply":"2024-06-26T09:25:06.193787Z"}}},{"cell_type":"markdown","source":"## Highlights\n\n* **Efficient and scalable**\n\n  * `accelerate` is the backbone of trl which allows to scale model training from a single GPU to a large scale multi-node cluster with methods such as `DDP` and `DeepSpeed`.\n  * `PEFT` is fully integrated and allows to train even the largest models on modest hardware with quantisation and methods such as **LoRA** or **QLoRA**.\n  * `unsloth` is also integrated and allows to significantly speed up training with dedicated kernels.\n\n* `CLI`: With the CLI you can fine-tune and chat with LLMs without writing any code using a single command and a flexible config system.\n\n* `Trainers`: The Trainer classes are an abstraction to apply many fine-tuning methods with ease such as the `SFTTrainer`, `DPOTrainer`, `RewardTrainer`, `PPOTrainer`, `CPOTrainer`, and `ORPOTrainer`.\n\n* `AutoModels`: The `AutoModelForCausalLMWithValueHead` & `AutoModelForSeq2SeqLMWithValueHead` classes add an additional value head to the model which allows to train them with **RL** algorithms such as *PPO*.\n\n* `Examples`: Train GPT2 to generate positive movie reviews with a BERT sentiment classifier, full RLHF using adapters only, train GPT-j to be less toxic, StackLlama example, etc. following the examples.\n","metadata":{}},{"cell_type":"markdown","source":"## Command Line Interface (CLI)\n\nYou can use TRL Command Line Interface (CLI) to quickly get started with Supervised Fine-tuning (SFT), Direct Preference Optimization (DPO) and test your aligned model with the chat CLI:","metadata":{}},{"cell_type":"markdown","source":"1. **SFT - Supervised Fine Tuning**\n```\n!trl sft --model_name_or_path facebook/opt-125m --dataset_name imdb --output_dir opt-sft-imdb\n```\n2. **DPO - Direct Preference Optimization**\n```\n!trl dpo --model_name_or_path facebook/opt-125m --dataset_name trl-internal-testing/hh-rlhf-helpful-base-trl-style --output_dir opt-sft-hh-rlhf\n```\n3. **Chat**\n```\n!trl chat --model_name_or_path Qwen/Qwen1.5-0.5B-Chat\n```\n\nThe above three commands we can use to Run & Train a model through `trl` command.\n\nI will run one chat command below for testing.","metadata":{}},{"cell_type":"code","source":"# !trl chat --model_name_or_path Qwen/Qwen1.5-0.5B-Chat # remove the comment and run","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to use\n\nFor more flexibility and control over the training, you can use the dedicated trainer classes to fine-tune the model in Python.","metadata":{}},{"cell_type":"markdown","source":"### SFTTrainer\n\nThis is a basic example of how to use the `SFTTrainer` from the library. The `SFTTrainer` is a light wrapper around the transformers Trainer to easily fine-tune language models or adapters on a `custom dataset`.\n\n**SFTTrainer** stands for `Supervised Fine-Tuning Trainer`. It's a class provided by the TRL package that facilitates the supervised fine-tuning of transformer models. This class helps in training a transformer model on a specific dataset using supervised learning techniques.\n\n**Main Use of SFTTrainer**\n\n1. `Fine-Tuning Pretrained Models`: SFTTrainer is primarily used to fine-tune pre-trained transformer models (e.g., BERT, GPT) on a specific dataset. Fine-tuning is a crucial step in adapting a general-purpose pre-trained model to a specific task, such as text classification, named entity recognition, or machine translation.\n2. `Supervised Learning`: It enables supervised learning by providing the necessary methods to train a model using labeled data. This involves defining a loss function, optimizing the model parameters, and evaluating the model's performance on validation data.\n3. `Customization`: SFTTrainer allows for customization of the training process. Users can specify their own loss functions, optimization algorithms, and other training parameters to suit their specific needs.\n\n**Where to Use SFTTrainer**\n\n1. `Natural Language Processing Tasks`: Any NLP task that can benefit from fine-tuning a transformer model can use SFTTrainer. Examples include sentiment analysis, text summarization, question answering, and more.\n\n2. `Custom NLP Pipelines`: If you're building a custom NLP pipeline and need to adapt a transformer model to your specific dataset or task, SFTTrainer can be an essential tool.\n\n3. `Research and Development`: For researchers experimenting with new models or techniques, SFTTrainer provides a flexible and easy-to-use framework for fine-tuning transformer models.\n\n","metadata":{}},{"cell_type":"markdown","source":" Class definition of the Supervised Finetuning Trainer (SFT Trainer).\nThis class is a wrapper around the `transformers.Trainer` class and inherits all of its attributes and methods.The trainer takes care of properly initializing the `PeftModel` in case a user passes a `PeftConfig` object.\n\n* **`model` (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`])**:\n The model to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the model name to load from cache or download. The model can be also converted to a `PeftModel` if a `PeftConfig` object is passed to the `peft_config` argument.\n\n* **`args` (Optional[`SFTConfig`]):**\n  The arguments to tweak for training. Will default to a basic instance of [`SFTConfig`] with the      `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n  \n* **`data_collator` (Optional[`transformers.DataCollator`]):** The data collator to use for training.\n* **`train_dataset` (Optional[`datasets.Dataset`]):**The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n* **`eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]):** The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n* **`tokenizer` (Optional[`transformers.PreTrainedTokenizer`]):** The tokenizer to use for training. If not specified, the tokenizer associated to the model will be used.\n* **`model_init` (`Callable[[], transformers.PreTrainedModel]`):** The model initializer to use for training. If None is specified, the default model initializer will be used.\n* **`compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to None):** The function used to compute metrics during evaluation. It should return a dictionary mapping metric names to metric values. If not specified, only the loss will be computed during evaluation.\n* **`callbacks` (`List[transformers.TrainerCallback]`):** The callbacks to use for training.\n* **`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):** The optimizer and scheduler to use for training.\n* **`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):** The function to use to preprocess the logits before computing the metrics.\n* **`peft_config` (`Optional[PeftConfig]`):** The PeftConfig object to use to initialize the PeftModel.\n* **`formatting_func` (`Optional[Callable]`):** The formatting function to be used for creating the `ConstantLengthDataset`.\n\n","metadata":{}},{"cell_type":"code","source":"# get staset\ndataset = load_dataset(\"imdb\", split=\"train\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}